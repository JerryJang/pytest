# pytest

# 개요 
이 프로젝트는 Speech Recognition 과목에서 진행한 '청각 장애인을 위한 서비스' 과제를 수행하기 위해 진행되었습니다. 
프로젝트 베토벤 이며 / 청각장애인을 위한 피아노 연주 도움 프로그램 입니다. 

프론트엔드(FE) 와 백엔드(BE) 두 부분으로 나누어 개발을 진행하였으며
각각 Git Reposigory에 업로드 하였습니다. 
이 레퍼지토리는 백엔드 개발을 한 내용을 기술하고 있습니다. 

참고로 프론트 엔트 주소는 다음과 같습니다. 
-> 

# 사용된 모듈 
> 파이썬 3.9.6 버전, 
> Django 프레임워크 를 사용했습니다. 
> librosa, pyaudio, portaudio, aubio 라이브러리 등 도 사용했습니다. 
> 기타 pip list 를 하고, 구현 과정 중 설치를 한 것 같은 리스트를 추려서 첨부합니다.
> 해당 라이브러리나, 코드 실행중 문제가 생긴다면 오류 메시지에 맞게 라이브러리를 설치하면 됩니다.
> 아키텍쳐 차이 ~ 윈도우, 맥 (Arm64과 Intel 기반에 지원이 되는 라이브러리와 되지 않는 라이브러리가 있다면
> 실행이 가능하게 되는 rosseta2, bosh 등의 확장도구를 사용하여야합니다. )

> Package                      Version
> ---------------------------- -----------
> aubio                        0.4.9
> audiofile                    1.4.0
> celery                       5.4.0
> cffi                         1.16.0
> channels                     4.1.0
> Django                       5.0.3
> djangorestframework          3.15.1
> librosa                      0.10.2
> numpy                        1.26.4
> PyAudio                      0.2.8
> sounddevice                  0.4.7
> virtualenv                   20.26.0
> wheel                        0.43.0

# 파일 트리 
<img width="380" alt="스크린샷 2024-06-19 오후 8 34 08" src="https://github.com/JerryJang/pytest/assets/25243469/82149b51-52a6-4f61-a709-fb7a9347c168">


# 함수 단위 설명 
1. detect_pitch_and_amplitude(samples, samplerate)
주어진 음성 샘플에서 피치와 진폭을 감지하는 비동기 함수입니다.

파라미터 : 
samples: 음성 샘플 데이터 배열
samplerate: 샘플레이트(초당 샘플 수)

동작 과정 :
aubio.pitch 객체를 생성하여 피치를 감지합니다.
샘플 데이터를 hop_s 단위로 분할하여 각 프레임의 피치와 진폭을 계산합니다.
진폭을 데시벨로 변환하여 반환합니다.

2. note_detect(samples, samplerate)
주어진 음성 샘플에서 주파수를 분석하여 음을 감지하는 비동기 함수입니다.

파라미터 : 
samples: 음성 샘플 데이터 배열
samplerate: 샘플레이트(초당 샘플 수)

동작 과정: 
샘플 데이터를 정규화합니다.
FFT(Fast Fourier Transform)를 사용하여 주파수 성분을 분석합니다.
주파수 피크를 찾아 음의 주파수를 계산하고, 주파수를 피아노 음으로 변환합니다 
(필요한 음을 모두 어레이 리스트로 나열하여서, 반복문으로 높낮음을 결정하여서 반환합니다. ) 

3. recognize_notes(websocket, path)
웹소켓을 통해 음성 데이터를 받아 이를 처리하는 비동기 함수입니다.

파라미터 :
websocket: 웹소켓 연결 객체
path: 웹소켓 경로
동작 과정

음성 데이터를 버퍼에 축적합니다.
버퍼가 샘플레이트 길이에 도달하면 데이터를 처리하여 피치와 진폭을 감지합니다.
감지된 음이 피아노 음 범위 내에 있으면 결과를 웹소켓을 통해 클라이언트로 전송합니다.


# 로직 구성 
음이 여러가지 들어오느 경우에 대해 고려해야 했습니다. 음악 연주는 한번의 POST 요청으로 종료되는 것이 아니라,
동작 버튼를 누르고, 따로 종료 버튼을 누르지 않은 한 계속하여서 음의 인식이 되어야 하기 때문입니다. 
처음에는 쓰레드를 사용하여서 각자의 요청을 받아 파일을 저장하고 별도처리를 하는 것으로 구상을 하였지만
나중에 웹소켓을 이용하여 비동기식(async)하게 처리를 하면 거의 실시간으로 sound processing 이 가능한 것을 알게되었습니다.
따라서 프론트에서 재생 요청이 오면 웹소켓을 열어서 소리를 실시간으로 받아 처리합니다. 사운드 해상도(44200, 샘플레이트) 등의 단위로 받아 
백엔드에 있는 recognize_note로 전송을 하면 buffer에 샘플레이트 수만큼 쌓이도록 기다렸다가 버퍼 길이가 차면 이 음성파일을 
note_detect 함수, detect_pitch_and_amplitude 에 전송하여서 음을 인식하도록 합니다. 
그리고 프론트에서는 연속으로 음이 일정 횟수 이상 연속으로 들어오게되면 이를 유효한 코드(valid Chaord)로 간주하고 프론트에서 처리합니다. 


# 작동에 관하여 (관리자 어드민) 
127.0.0.1:8000/admin 으로 접속하면 관리자 계정을 로그인 하도록 되어 있습니다.
계정 정보는 사용자 설정에 따르고, 
<img width="801" alt="스크린샷 2024-06-19 오후 8 15 24" src="https://github.com/JerryJang/pytest/assets/25243469/515b71e9-6e55-4027-8949-41b9de36897c">
장고 model.py 에서 작성한 ERD 를 확인할 수 있습니다. 실제 작동한다면 (악보나, 음성인식) 파일을 업로드하면 다음과 같은 경로에 추가되어 서버PC에 저장이 됩니다. <img width="309" alt="스크린샷 2024-06-19 오후 8 16 51" src="https://github.com/JerryJang/pytest/assets/25243469/dec060b9-6939-4227-a94a-574c442f71b9">
> pytest/media/{각자 경로}  /'audio' 음악파일 , 'image'악보타이틀 이미지, 'sheet_music'악보 

# 작동에 관하여 (테스트)
1. 실행
콘솔 창이 2개 필요합니다. 우선 경로에 맞게 들어갑니다.
첫번째 경로는 ~/testproject/ 입니다.
> python3 manage.py runserver


두번째 경로는 ~/testproject/blog/
> python3  python3 websocket_server.py

2. 웹 접속
자, 실행에 성공하였다면 장고는 127.0.0.1:5000 (웹소켓 서버) / 127.0.0.1:8000 (장고 메인페이지) 에서 접속이 가능합니다.
이건 백엔드 로직이 잘 작동하는지를 확인하기 목적이고, 실제 서비스적인 동작은 FE 파트에서 진행됩니다.

HTML 파일에 따라 다르긴 하지만, 이런상태로 음이 인식되는 것을 확인 할 수 있습니다. (예전버전 스크린샷이예요)
<img width="569" alt="스크린샷 2024-06-12 오후 9 16 59" src="https://github.com/JerryJang/pytest/assets/25243469/2818f37e-8f05-4ed4-a54c-cae33df8f7c5">


# 어려웠던 점?
1. 맥과 윈도우 간 아키텍쳐 차이로 인해 라이브러리들이 어떤 것은 윈도우에서 실행되지만 맥에서는 실행이 안되고, 맥에서는 실행되지만 윈도우에서는 안되는 그런 문제가 있었습니다. 오류가 왜 발생하는지 에러메시지를 하나씩 읽어보면서 꽤 많은시간을 보냈고, 환경 차이에 의해서 발생하였음을 알게 되었습니다,

2. 최초 시연 과정 중에 제대로 음이 전달되지 않는 상황, 단일음 테스트에서는 문제가 없는 것으로 보였으나 연주 상황에서는 인식이 실시간으로 안되거나, 음이 튀는 그런 상황이 타났습니다. 이는 오디오파일의 인코딩과 샘플레이트 수가 제대로 되지 않아 fft을 적용하고 amplitute를 가져오는 상황에서 여러 문제가 발생하였던 것으로 보여집니다. 현재는 수정하여서 정상작동합니다.

3. (아쉬움) 현재는 단일음 연주에 대해서만 가능합니다. 라이브러리에 의존하여서 구현을 하다보니 부딪히는 한계인것 같습니다. 하지만 실제 피아노 연주는 양손을 다 사용하기에 동시에 여러 음이 눌려서 들어왔을 때에도 이를 인식할 수 있어야 진정한 서비스가 가능하다고 생각이 됩니다. 음원 인식데이터를 많이 모아서 딥러닝, 머신러닝를 진행하여 여러가지 음에 대한 인식이 가능하도록 더 공부를 해보고 싶습니다. 향후 서비스를 발전 시켜보면 좋을 것 같다는 생각이 되어집니다. 


